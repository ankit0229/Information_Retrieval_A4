{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import copy\n",
    "import bz2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "words_list1 = []\n",
    "directory = r'20_newsgroups\\\\'\n",
    "for entry in os.listdir(directory):\n",
    "    if os.path.isdir(os.path.join(directory, entry)):\n",
    "        files.append(entry)\n",
    "\n",
    "docs = []\n",
    "directory = r'20_newsgroups\\\\'\n",
    "\n",
    "mstr_dict_dict = {}\n",
    "all_doc_ids = []\n",
    "list_vocab_words = []\n",
    "dict_folder_docs = {}\n",
    "\n",
    "for fol in files:\n",
    "    temp_dir = os.path.join(directory, fol)\n",
    "    docs = []\n",
    "    complete_doc_loc = []\n",
    "    for entry in os.listdir(temp_dir):\n",
    "        if os.path.isfile(os.path.join(temp_dir, entry)):\n",
    "            docs.append(entry)\n",
    "            doc_loc = os.path.join(fol, entry)\n",
    "            complete_doc_loc.append(doc_loc)\n",
    "\n",
    "    all_doc_ids.extend(complete_doc_loc)\n",
    "    dict_folder_docs[fol] = complete_doc_loc\n",
    "    \n",
    "    for dd in range(len(docs)):\n",
    "        doc = docs[dd]\n",
    "        full_path = os.path.join(temp_dir, doc)\n",
    "        fp = open(full_path, \"r\")\n",
    "        text = fp.read()\n",
    "        fp.close()\n",
    "        ll = text.split(\"\\n\\n\")\n",
    "        del ll[0]\n",
    "        text = \"\\n\\n\".join(ll)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "        text = re.sub(r'[a-zA-Z]+[0-9]+', '', text)\n",
    "        text = re.sub(r'[0-9]+[a-zA-Z]+', '', text)\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))  # map punctuation to space\n",
    "        text = text.translate(translator)\n",
    "#         text = re.sub(r'\\d+', '', text)\n",
    "        word_tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [word for word in word_tokens if word not in stop_words]\n",
    "#         lemmas = [ps.stem(word) for word in text]\n",
    "        lemmas = [lemmatizer.lemmatize(word) for word in text]\n",
    "        dict_doc = Counter(lemmas)\n",
    "        temp_set = set(lemmas)\n",
    "        unq_wrds = list(temp_set)\n",
    "        list_vocab_words.extend(unq_wrds)\n",
    "#         doc_location = os.path.join(fol, doc)\n",
    "        doc_location = complete_doc_loc[dd]\n",
    "        mstr_dict_dict[doc_location] = copy.deepcopy(dict_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_set = set(list_vocab_words)\n",
    "list_vocabulary = list(temp_set)\n",
    "dict_nt = {}\n",
    "for a in list_vocabulary:\n",
    "    dict_nt[a] = 0\n",
    "    for b in mstr_dict_dict:\n",
    "        curr_dict = mstr_dict_dict[b]\n",
    "        if curr_dict[a] != 0:\n",
    "            dict_nt[a] = dict_nt[a] + 1\n",
    "\n",
    "list_nt_values = []\n",
    "for ab in dict_nt:\n",
    "    list_nt_values.append(dict_nt[ab])\n",
    "\n",
    "max_nt = max(list_nt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_docs = len(all_doc_ids)\n",
    "count_vocab = len(list_vocabulary)\n",
    "# data_matrix_vectors = [[0 for i in range(count_vocab)] for j in range(count_docs)]\n",
    "dict_matrix_vectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(count_docs):\n",
    "    curr_doc = all_doc_ids[c]\n",
    "    curr_doc_dict = mstr_dict_dict[curr_doc]\n",
    "    dict_matrix_vectors[curr_doc] = []\n",
    "    for d in range(count_vocab):\n",
    "        curr_wd = list_vocabulary[d]\n",
    "        val_tf_log = math.log10(1 + curr_doc_dict[curr_wd])\n",
    "        val_idf_max = math.log10(count_docs / (1 + dict_nt[curr_wd]))\n",
    "        prod = val_tf_log * val_idf_max\n",
    "        dict_matrix_vectors[curr_doc].append(prod)\n",
    "#         data_matrix_vectors[c][d] = prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picklefile3 = open('data_vocabulary', 'wb')\n",
    "pickle.dump(list_vocabulary, Picklefile3)\n",
    "Picklefile3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picklefile1 = bz2.open('data_vectors', 'wb')\n",
    "pickle.dump(dict_matrix_vectors, Picklefile1)\n",
    "Picklefile1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picklefile2 = open('dictionary_nt', 'wb')\n",
    "pickle.dump(dict_nt, Picklefile2)\n",
    "Picklefile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
